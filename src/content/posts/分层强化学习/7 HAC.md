---
title: 【分层强化学习】#7 HAC：从双层到多层的高效实现
published: 2025-11-30
description: 稳定、高效且安全的多层并行学习。
tags: [HAC, 子目标测试, 事后经验回放, Actor-Critic]
category: 分层强化学习
---
在深度分层强化学习的早期发展过程中，大多数算法均停留在双层架构的设计，而未能向真正的多层取得突破。其核心原因在于，当层次超过两层时，会引发一系列连锁反应，放大分层强化学习的诸多缺陷，使得简单地扩展双层架构变得极其困难：

- 非平稳性：低层策略的同时更新会带来高层策略的动作变得非平稳，这一问题在HIRO开篇介绍过，HIRO也提供了一个重要的解决思路；
- 信用分配级联：最高层的决策需要经过很多步才能最终影响到环境，如果任务失败，将很难判断是哪一个中间层的子目标有误；
- 高层训练信号稀疏：高层策略极低的行动频率意味着其获得反馈的间隔极其漫长，学习速度缓慢。

# HAC

**HAC**（Hierarchical Actor-Critic，分层演员-评论家）的提出正是为了系统性解决深度分层强化学习从双层到多层面临的难题。它在HIRO的基础上，进一步优化了经验复用的方法，并引入子目标测试机制，使得高层策略的训练更加稳定而高效，通过一整套工程解决方案，最终实现了真正有效的多层策略学习架构。

## 分层架构与嵌套策略

HAC使用**UMDP**（Universal MDP，通用马尔可夫决策过程）来定义分层强化学习任务。一个UMDP由一个六元组$\mathcal U=(\mathcal S,\mathcal G,\mathcal A,T,R,\gamma)$描述，元组内元素依次为状态空间$\mathcal S$、目标空间$\mathcal G$、动作空间$\mathcal A$、状态转移函数$T$、奖励函数$R$和折扣率$\gamma$。

HAC分层架构包含$k$个层级，每个层级有：

- 各自的UMDP$\mathcal U_0,\cdots,\mathcal U_{k-1}$；
- 独立的策略$\pi_0,\cdots,\pi_{k-1}\in\Pi_{k-1}$，$\pi_i:\mathcal S_i,\mathcal G_i\rightarrow\mathcal A_i,0\leq i\leq k-1$，其中：
	- 状态空间与原始问题的状态空间一致：$\mathcal S_i=\mathcal S$；
	- 目标空间与状态空间一致：$\mathcal G_i=\mathcal S$；
	- 高层策略$\pi_i(1\leq i\leq k-1)$输出的动作为目标，即动作空间与状态空间也一致：$\mathcal A_i=\mathcal S$；
	- 底层策略$\pi_0$输出原始动作，其动作空间与原始动作空间一致：$\mathcal A_0=\mathcal A$。

![Pasted image 20251130150817](https://inkem-1306784622.cos.accelerate.myqcloud.com/blog/pic/Pasted%20image%2020251130150817.png)

HAC通过**嵌套策略**实现问题分解，一个自上而下的决策流程为：

1. 高层策略$\pi_i$根据当前目标$g_i$和状态$s_i$，输出动作$a_i$；
	1. 第$i-1$层接收来自第$i$层的动作$a_i$作为其目标$g_{i-1}$；
	2. 低层策略$\pi_{i-1}$根据目标$g_{i-1}$和状态$s_{i-1}$，输出动作$a_{i-1}$；
		1. $\cdots$
			1. 第$0$层接收来自第$1$层的动作$a_1$作为其目标$g_0$；
			2. 底层策略$\pi_0$根据目标$g_0$和状态$s_0$，输出动作$a_0$由智能体执行。

决策自下而上的返回则依赖于高层$i$状态转移$T_i$的结束条件，会在以下两种情况被低层$i-1$触发：

- 低层策略$\pi_{i-1}$执行了$H$步仍未达成来自高层的目标$g_{i-1}$；
- 低层策略$\pi_{i-1}$在$H$步内达成了来自高层的目标$g_{i-1}$，或者意外达成了更高层的目标$g_k(k>i-1)$。

$H$是每一层进行决策的固定时间跨度。

## 事后动作转移

HAC的多层嵌套策略将在训练中导致更严重的非平稳性。一方面低层策略的更新会带来高层策略所处环境的变化；另一方面HAC的每一层采用确定性策略，低层策略在训练时为了探索会使用带高斯噪声的行为策略$\pi_{ib}=\pi_i+\mathcal N(0,\sigma^2)$，即使参数固定，高层策略也会观察到随机且不一致的结果。

HAC通过一个巧妙的假设为高层策略人为创造了一个稳定的理想化的学习环境。对于每个高层策略$\pi_i$，HAC假设其下属的所有低层策略$\Pi_{i-1}$都是最优策略$\Pi_{i-1}^*$来屏蔽非平稳性。这一假设由**事后动作转移**（Hindsight Action Transition）来实现，其思想与HIRO的离策略校正一脉相承。我们考虑一个简单的双层HAC架构，它的任务是指导智能体到达黄旗的位置。

![Pasted image 20251129173107](https://inkem-1306784622.cos.accelerate.myqcloud.com/blog/pic/Pasted%20image%2020251129173107.png)

在状态$s_0$，高层策略$\pi_1$给出目标$g_0$，低层策略$\pi_0$执行$H=5$步后到达$s_1$，再由高层策略$\pi_1$给出目标$g_1$，以此类推。一次状态转移$T$产生的经验是由转移前状态$s$、输出动作$a$、转移结束时状态$s'$、来自高层的目标$g$和未来价值折扣率$\gamma$组成的五元组$(s,a,s',g,\gamma)$。以$g_0$产生的状态转移为例，高层策略$\pi_1$收集的经验为$(s_0,a=g_0,r,s'=s_1,g_{total}=黄旗,\gamma)$。

事后动作转移的关键是动作重构，其核心思想是“做了什么”比“想做什么”更重要。我们可以认为，对于高层策略$\pi_1$给出的任何理论可行的目标，一个最优低层策略$\pi_0^*$应当都能够在$H$步内抵达。如果$\pi_0$在实际执行过程中实现了这一点，那么我们自然可以视这条经验来自于最优低层策略$\pi_0^*$。如果$\pi_0$实际执行失败了，如图中智能体从$s_0$开始按目标$g_0$执行却到达了较远的$s_1$，HAC将进行事后处理，修正经验中的动作$g_0$为$s_1$。如此一来，原先在目标$g_0$下由$\pi_0$产生的失败经验，被转化为了在目标$s_1$下可以被视为由$\pi_0^*$产生的成功经验，高层策略$\pi_1$也将学习在$s_0$选择去$s_1$而非$g_0$对于达成黄旗目标的价值。

经验中奖励函数$r$的设计遵循两条原则。一是我们只能保证最优低层策略$\pi_0^*$可以达成的结果，而无法预知其具体会走哪条路径，因此奖励函数应与低层策略执行路径无关；二是奖励函数应激励高层策略找到最短路径。为此，HAC采用了一个极其简单的稀疏奖励，即如果达到的状态等于本层的目标（或足够接近），则奖励$r=0$，否则$r=-1$。

最终，$\pi_1$在$s_0$输出动作$g_0$后到达$s_1$产生的经验为$(s_0,a=s_1,r=-1,s'=s_1,g_{total}=黄旗,\gamma)$。

## 事后目标转移

事后动作转移通过修正高层策略输出给低层策略的动作$a$，保证了高层策略的训练稳定性。而**事后目标转移**（Hindsight Goal Transition）则通过修正当前层的目标$g$，通过缓解稀疏奖励问题以保证每一层策略的训练效率。

仍以黄旗任务为例，图中高层策略$\pi_1$在一轮决策完毕时，共收集了从$s_0$到$s_5$的$5$条原始经验，但是如果我们直接使用原始经验中的$g_{total}=黄旗$来训练策略$\pi_1$，那么它们都会因为没有达到$g_{total}$而产生负奖励$r=-1$，这样的稀疏奖励将导致$\pi_1$无法学到成功的经验。为了让$\pi_1$能学到至少一条成功的经验，HAC将当前层目标修正为$s_5$，由此从$s_4$到达$s_5$产生的经验会变为$r=0$的成功经验，在$g_{total}=黄旗$下的$5$条失败经验被重标定为了在$\tilde g_{total}=s_5$下的$4$条失败经验和$1$条成功经验。这样一来，即使低层策略永远无法在$H$步内达到高层设定的目标，它也能通过事后目标转移不断获得关于达成可实现目标的成功经验，为网络提供了稳定的学习梯度。

在具体算法实现中，当发生一次状态转移时，HAC会同时创建两条经验，一条为基于原始目标的**标准经验**，另一条为奖励和目标被擦除（用TBD表示），等待事后处理填充的**待定经验**。当收集完$H$步产生的所有经验后，HAC将随机从这$H$步的每一步到达的状态中随机选$n$个状态作为当前层的$n$个目标，再将每个新目标分别填充到所有经验的目标中并计算奖励，生成$H\times n$条**事后经验**，其中将至少包含$n$条成功经验。原始的标准经验也会用于训练策略，以防其偏离真实的任务目标。

诸如HIRO的离策略校正、HAC的事后动作转移和事后目标转移这类利用“重新解释失败经历”机制对经验进行修正的方法，被称为**事后经验回放**（Hindsight Experience Replay，HER），有效地解决了深度分层强化学习中的稀疏奖励问题。

## 子目标测试

仅凭事后动作转移和事后目标转移将带来两个缺陷，它们均源于高层策略对低层策略的认知与实际情况出现了偏差：

- 高层策略$\pi_i$将只能学习到当前低层策略$\pi_{i-1}$在$H$步内能实现的子目标（动作）的Q值，而忽略更遥远的子目标的Q值学习。并且随着训练推移，即便$\pi_{i-1}$可达范围扩大，$\pi_i$也无法及时认识到这种能力增长。一旦网络初始化对初期$\pi_{i-1}$可达范围外的子目标产生了高估或低估，$\pi_i$将难以纠正其偏差，从而导致智能体行为不稳定；
- 高层策略$\pi_i$可能会执着于在之前的尝试中发现的最短路径，然而低层策略的能力是动态变化的，可能因探索和泛化而遗忘某些特定技能，导致这条最短路径在当前时刻无法实现，这对高层策略而言也不能被有效地觉察。

尽管Critic网络对Q值的学习具有一定的泛化能力，即可以通过学习当前低层策略可达范围边界的Q值间接地、模糊地学习到稍远的Q值，这对高层策略与低层策略之间信息对齐的帮助也较为有限。为了缓解这一偏差，HAC提出了**子目标测试**（Subgoal Testing Transitions）来帮助高层策略跟进对低层策略可达范围的学习。当高层策略$\pi_i$选择了子目标$a$后，会有$\lambda$的概率进入测试模式。在测试模式下，下属低层策略$\Pi_{i-1}$将关闭探索噪声，严格遵循当前的学习策略执行。测试模式将根据不同的执行结果产生经验：

- 如果低层策略$\pi_{i-1}$在$H$步内成功达成子目标，则一切照常，不产生特殊经验；
- 如果低层策略$\pi_{i-1}$在$H$步内失败了，则为高层策略$\pi_i$创建一条子目标测试经验$(s,a,r=-H,s',g,\gamma=0)$，其关键在于：
	- 奖励$r=-H$给出严重惩罚，相当于浪费了整个时间跨度；
	- 折扣率$\gamma=0$不考虑未来状态价值，即这是一个固定的即时收益，不会因为未来状态价值随着低层策略学习改变而改变。

下面我们分三种情况来理解子目标测试如何塑造Q值：

- 绝对不可达的子目标，即最优低层策略也无法在$H$步内达成的子目标。此时子目标测试会稳定地提供$-H$的惩罚，使Q值收敛到$-H$，让高层策略避开；
- 当前即可达的子目标。此时子目标测试通常会成功，Q值主要由事后经验决定，不受子目标测试干扰；
- 理论可达但当前无法实现的子目标，即最优低层策略可达，但当前低层策略做不到。此时Q值是大量事后经验泛化学习和少量子目标测试经验惩罚的加权平均（测试经验权重收敛于测试概率$\lambda$），并随着低层策略逐渐扩大可达范围，最终能熟练达成该子目标时，Q值将上升到其理论值。

总而言之，HAC以嵌套策略为框架，通过事后动作转移解决层级间的非平稳性问题，通过事后目标转移解决层级内的稀疏奖励问题，再利用子目标测试解决前两者导致的高层策略对低层策略的认知偏差，实现了稳定、高效且安全的多层并行学习。

## 算法流程

对于每一层策略Actor网络和价值函数Critic网络的训练，HAC采用了DDPG算法。HAC算法的伪代码如下：

1. 初始化：
	1. 超参数：层级数量$k$，最大时间跨度$H$，子目标测试概率$\lambda$，折扣因子$\gamma$；
	2. 每一层的Actor网络和Critic网络。
2. 对每一幕循环：
	1. 初始化状态$s\leftarrow S_{init}$和总目标$g\leftarrow G_{k-1}$；
	2. 运行训练函数$train\_level(k-1,s,g)$；
	3. 根据DDPG算法并行更新所有Actor网络和Critic网络。

其中训练函数$train_level(i,s,g)$是一个递归函数，其伪代码如下：

1. 输入层级$i$，状态$s$，目标$g$；
2. 初始化状态$s_i\leftarrow s$和目标$g_i\leftarrow g$；
3. 循环$H$步或直到实现高层目标$g_n,i\leq n<k$：
	1. 输出动作$a_i\leftarrow\pi_i(s_i,g_i)+noise(非子目标测试)$；
	2. 如果$i>0$：
		1. 决定是否测试子目标$a_i$；
		2. 获取后继状态$s_i{'}\leftarrow train\_level(i-1,s_i,a_i)$。
	3. 否则：
		1. 执行原始动作$a_0$并观测后继状态$s_0{'}$。
	4. 如果$i>0$并且未实现$a_i$：
		1. 如果$a_i$被测试：
			1. 将测试经验$(s=s_i,a=a_i,r=-H,s'=s_i{'},g=g_i,\gamma=0)$存入经验缓存区。
		2. 事后动作转移$a_i\leftarrow s_i{'}$。
	5. 将标准经验$(s=s_i,a=a_i,r\in\{-1,0\},s'=s_i{'},g=g_i,\gamma=\gamma)$存入经验缓存区；
	6. 将待定经验$(s=s_i,a=a_i,r=TBD,s'=s_i{'},g=TBD,\gamma=\gamma)$存入待定经验缓存区；
	7. 更新状态$s\leftarrow s_i{'}$。
4. 事后目标转移：将待定经验缓存区中的待定经验填充为事后经验存入经验缓存区。
5. 返回$s_i{'}$。