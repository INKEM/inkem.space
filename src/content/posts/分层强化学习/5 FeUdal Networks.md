---
title: 【分层强化学习】#5 FeUdal Networks：管理者-工人架构
published: 2025-11-18
description: 端到端架构与分离学习的结合。
tags: [FuN, 策略梯度, RNN]
category: 分层强化学习
---
Option-Critic架构以其优雅的端到端训练框架，使得任务的子目标可以直接由网络学习，无需进行人为设计。然而其代价便是高层策略输出的目标沦为了训练过程中难以捉摸的黑箱变量，我们只能依赖外部奖励引导整个网络的训练，无法进一步对高层策略和底层策略进行区别干预。FeUdal Networks则为目标赋予了一个更清晰的语义，在保留端到端架构的前提下，通过精巧的机制实现了高层策略和底层策略的分离学习。

# FeUdal Networks

**FeUdal Networks**（封建网络，简称FuN）是一个模块化神经网络，包括工人Worker和管理者Manager两个模块。管理者负责高层决策，向工人输出目标；工人负责底层执行，结合管理者的目标输出动作。

## 管理者-工人架构

FuN的网络架构与此前介绍的算法有显著区别，主要在于

- 管理者拥有自己的状态空间转换网络，用于从环境中提取出有别于工人的、用于长期战略规划的抽象状态作为输出目标的依据；
- 管理者和工人的策略网络采用循环神经网络（RNN）。RNN在每个时间步接收当前观测和网络上一时间步的记忆作为输入，输出新的记忆和决策结果。RNN使得智能体能够利用从过去到现在所有观测的历史信息来进行决策，推断那些无法从当前状态直接看到的信息，例如玩扑克牌需要记住之前出过的牌来推断对手可能的手牌；
- 管理者决策的时间步间隔是固定的，而非等待子目标终止后决策。在基于目标终止条件的高层决策中，终止条件的学习带来了更大的训练难度，并让信用分配更加复杂。FuN通过固定决策间隔，以牺牲目标的灵活性为代价换来了更稳定的训练，并且让高层可以给予底层持续的引导，防止底层在错误的方向上走得太远；
- 工人并非直接根据当前状态和目标给出动作，而是先依赖当前状态输出一个中间矩阵，再与目标结合给出动作。这个过程将问题进一步分解，即工人不需要直接学习从状态-目标对到动作的复杂映射，而是可以被理解为只学习某个状态下的行动空间，包含所有可能的行动方案，目标则作为权重再对这些行动方案进一步筛选得到最终的动作。这种解耦可以引导模型学习一种结构化的策略空间，相比直接学习从状态-目标对到动作的复杂函数更加简单且易于优化。

现在我们来详细展开FuN中前向传播的每一步：

![QQ_1763436052075|500](https://inkem-1306784622.cos.accelerate.myqcloud.com/blog/pic/QQ_1763436052075.png)

**①环境感知网络**$f^{percept}$（可选），用于从复杂高维的原始环境信息$x_t$中提取特征表示$z_t$：

$$
z_t=f^{percept}(x_t)
$$

论文中使用卷积神经网络处理像素图像。如果环境可以由人工设计的低维状态空间表示，则这一步可以省略。

**②管理者状态空间转换网络**$f^{Mspace}$，用于从特征$z_t$进一步提取出更高层的潜在状态$s_t$：

$$
s_t=f^{Mspace}(z_t)
$$

**③管理者RNN**$f^{Mrnn}$，根据潜在状态$s_t$制定当前的目标$g_t$：

$$
\begin{split}
&h_t^M,\hat g_t=f^{Mrnn}(s_t,h_{t-1}^M)\\
&g_t=\frac{\hat g_t}{\|\hat g_t\|}
\end{split}
$$

其中$h_{t-1}^M$和$h_t^M$分别为管理者RNN上一个时间步和当前时间步的隐藏状态，$\hat g_t$为管理者输出的原始目标向量。最终的目标向量$g_t$由$\hat g_t$归一化得到，从而让管理者只关心目标的方向而不关心大小。

**④目标嵌入**$\phi$，用于将目标转化为工人能理解的指令（作用方式见⑥）：

$$
w_t=\phi(\sum^t_{i=t-c}g_i)
$$

其中$\phi$是一个线性转换层，$w_t$是嵌入后的目标向量。求和操作可以平滑管理者每隔$c$个时间步给出新目标时的目标跳变。如果在工人按照旧目标执行$c$步后突然切换为新目标，会带来训练不稳定和智能体行为不连贯的问题。目标嵌入通过对一定时间步范围内的目标求和，可以平缓地从旧目标过渡到新目标。自管理者给出新目标后，新目标的权重会随着时间推移越来越大。

![Pasted image 20251117213058|500](https://inkem-1306784622.cos.accelerate.myqcloud.com/blog/pic/Pasted%20image%2020251117213058.png)

**⑤工人RNN**$f^{Wrnn}$，根据特征$z_t$输出一个矩阵$U_t$：

$$
h_t^W,U_t=f^{Wrnn}(z_t,h_{t-1}^W) 
$$

$U_t$被设计为工人根据当前状态列出的$k$个行为模式，每个行为模式包含对所有动作的偏好值，可以理解为未经归一化处理的策略概率分布。$U_t$大小为$|a|\times k$，$|a|$为动作数量。

**⑥最终策略**$\pi_t$，由工人的输出矩阵$U_t$和管理者的目标嵌入$w_t$相乘后进行softmax操作得到：

$$
\pi_t=SoftMax(U_tw_t)
$$

$w_t$大小为$k\times1$，通过与$U_t$相乘对所有的行为模式进行加权求和得到最终的行为模式，softmax操作将其转化为所有动作的概率分布。

## 管理者策略梯度

整个FuN架构是可微分的，理论上可以用策略梯度算法进行纯粹的端到端训练。但是如果只通过工人执行的动作产生的梯度来反向传播训练管理者，那么目标$g_t$就会沦为一个黑箱中的内部信号，可能无法学习到我们期望的“目标”的语义。为此，FuN赋予了目标“状态转移的方向”这一内涵，设计了一个独立的训练信号来训练管理者预测状态空间中“有利的方向”作为目标向量的方向，其更新规则如下

$$
\nabla g_t=A_t^M\nabla_{\theta^M}d_{\cos}(s_{t+c}-s_t,g_t(\theta^M))
$$

- $A_t^M=R_t-V_t^M(x_t,\theta_1)$：管理者的优势函数，用于衡量在状态$x_t$下，遵循当前管理策略的好坏程度。其中：
	- $R_t$：从时刻$t$开始的累积奖励；
	- $V_t^M(x_t,\theta^{V_M})$：管理者评论家网络对状态$x_t$的价值估计，$\theta^{V_M}$为网络参数。
- $d_{\cos}(\alpha,\beta)=\alpha^T\beta/(|\alpha||\beta|)$：余弦相似度，用于衡量向量$\alpha$与$\beta$的方向有多接近，在式中的参数为：
	- $s_{t+c}-s_t$：在潜在状态空间中，从当前时刻$t$到未来$c$步后的实际状态变化量；
	- $g_t(\theta^M)$：管理者在时刻$t$预测的、希望工人实现的目标方向。管理者参数$\theta^M$是从$x_t$到$g_t$所经历的所有网络的参数，包括感知器参数$\theta^{percept}$、管理者状态空间转换网络参数$\theta^{Mspace}$和管理者RNN参数$\theta^{Mrnn}$。该更新规则不考虑梯度对潜在状态$s$的依赖，因此主要对$\theta^{Mrnn}$起更新作用。

总而言之，管理者的更新规则是一个双目标优化过程：优势函数$A_t^M$让目标沿着能带来更高回报的方向更新，而余弦相似度梯度$\nabla_{\theta^M}d_{\cos}(s_{t+c}-s_t,g_t(\theta^M))$保证管理者给出的目标$g_t$能够准确地指导工人的行为，进而牵引状态转移。二者共同作用让目标的更新有利且有效。

管理者将在每一个决策点之前，即一个目标周期结束时更新参数，以确保其更新所用序列均基于计算梯度所用的目标。

## 工人策略梯度

FuN为工人设置了一个内在奖励作为其更新目标之一

$$
r_t^I=\frac1c\sum^c_{i=1}d_{\cos}(s_t-s_{t-i},g_{t-i})
$$

该内在奖励是对过去$c$步，也就是每个目标的作用时间内，状态转移和目标的余弦相似度的平均。具体而言，其衡量的是对于管理者在$t-i$时刻给出的目标$g_{t-i}$，工人在经过$i$步后，产生的状态转移与之有多一致。与此前介绍的算法相比，FuN不要求工人到达某个特定状态，而是奖励它朝着正确的方向改变状态，进而鼓励了持续的目标导向的探索。

工人的内在奖励和管理者的更新规则都涉及状态转移和目标的余弦相似度，这体现了一种对称性。管理员调整目标确保其被正确传达给工人，而工人则调整策略确保能正确执行管理者的目标，最终达成高层决策与底层执行的相统一。

FuN的又一创新举措是采用混合奖励信号训练工人，让工人不仅通过内部奖励服从管理者的目标，也要看到外部奖励以理解任务的终极目标来微调行为。工人的总奖励是外部奖励和内部奖励的加权和

$$
R^D_t=R_t+\alpha R^I_t
$$

其中$R_t$为累积外部奖励，$R_t^I$为累积内部奖励，超参数$\alpha$用于平衡两种奖励的重要性。

工人参数$\theta^W$，包括感知器参数$\theta^{percept}$、工人RNN参数$\theta^{Wrnn}$和目标嵌入层参数$\theta^\phi$的更新基于A2C算法，即采用优势策略梯度作为更新规则

$$
\nabla\pi_t=A_t^D\nabla_{\theta^W}\log\pi(a_t|x_t;\theta^W)
$$

其中工人的优势函数$A^D_t$定义为

$$
A^D_t=R^D_t-V_t^D(x_t;\theta^{V_W})
$$

$V_t^D(x_t;\theta^{V_W})$是工人评论家网络，其参数为$\theta^{V_W}$。

工人和管理者在计算累积奖励时可以采用不同的折扣因子$\gamma$，让工人更短视以专注眼前的回报，而管理者则考虑长期回报。

## 转移策略梯度

FuN提出了转移策略，进一步给出了管理者和工人采用上述策略梯度的理论基础。我们记管理者策略为$g_t=\mu(s_t,\theta^M)$，而工人的行为对管理者体现为一个状态转移分布$p(s_{t+c}|s_t,g_t)$，这个分布描述了在状态$s_t$下，如果管理者给出了目标$g_t$，那么经过$c$步后状态$s_{t+c}$的概率分布是怎样的。将管理者策略和这个状态转移分布结合，可以定义一个新的策略

$$
\pi^{TP}(s_{t+c}|s_t)=p(s_{t+c}|s_t,\mu(s_t,\theta^M))
$$

该策略即为**转移策略**，它输出的不是动作，而是$c$步后的状态分布。在原始的MDP上学习管理者策略，等价于在一个新的MDP上学习这个转移策略，其中新MDP的每一步对应原始MDP中的$c$步。因此转移策略是一个有效的策略，可以应用策略梯度定理。

博主的理解是，原始策略$\pi(a|s)$输出的动作分布本质上也和环境的动态特性$p(s'|s,a)$共同决定了一个状态转移分布

$$
p_\pi(s'|s)=\sum_a\pi(a|s)p(s'|s,a)
$$

而在转移策略中，这个逻辑被反转了。转移策略输出的状态转移分布，实际上也隐式地指向了一个动作序列的分布$(a_0,a_1,\cdots,a_{c-1})$。这个指向关系难以由计算得到，因此在FuN中，它成为了工人的学习任务，也就是学习如何在$c$步内采取一系列动作以实现管理者期望的状态转移分布，而管理者只需从宏观的状态转移视角作出决策。换言之，管理者不会告诉工人“怎么做”，只会要求“实现什么目标”。

显然，转移策略的策略梯度定理是有条件的，它要求工人有能力满足管理者的要求，能以管理者给出的目标为中心落实状态转移。从数学上来讲，即对于当前状态$s_t$和管理者给出的目标$g_t$，在工人$c$步后可达的状态集合中，存在一些状态$s_{t+c}$能够符合转移策略给出的分布$p(s_{t+c}|s_t,g_t)$。

FuN选择VMF分布作为转移策略$\pi^{TP}(s_{t+c}|s_t)$输出的分布

$$
p(s_{t+c}|s_t,g_t)\propto e^{d_{\cos}(s_{t+c}-s_t,g_t)}
$$

我们可以将其理解为球面上的高斯分布，正如我们令一个原始策略$\pi(a|s)$以高斯分布作为输出一样。采用球面是因为我们关注的是状态转移的方向。在该分布下，转移策略的策略梯度形式将符合管理者参数的更新规则

$$
\nabla g_t=A_t^M\nabla_{\theta^M}d_{\cos}(s_{t+c}-s_t,g_t(\theta^M))
$$

实际上FuN正是基于该转移策略对管理者的更新规则进行推导的，而工人的内部奖励函数的设计也是为了满足转移策略能应用于策略梯度定理的条件。

## 算法流程

我们将感知器和管理者状态空间转换网络统一记为编码器$f^{encoder}(x_t;\theta^{enc})$，负责将$x_t$编码为$z_t$和$s_t$，其参数将通过工人和管理者的梯度留进行更新。

最终我们给出FuN的一个算法伪代码如下：

（*原论文没有给出伪代码，博主自行给出，仅供参考*）

1. 初始化：
	1. 管理者策略网络$\mu(s_t;\theta^M)$及其评论家$V_t^M(x_t,\theta^{V_M})$；
	2. 工人策略网络$\pi(a_t|z_t,w_t;\theta^W)$及其评论家$V^W(x_t;\theta^{V_W})$；
	3. 编码器$f^{encoder}(x_t;\theta^{enc})$；
	4. 目标嵌入层$\phi$；
	5. 管理器时间尺度$c$，折扣因子$\gamma^M,\gamma^W$，内部奖励权重$\alpha$；
	6. 经验存放缓存区$\mathcal D$（更新网络默认从缓存区中随机抽取一个批量进行更新）。
2. 对每一幕循环：
	1. 初始化观测$x\leftarrow x_0$；
	2. 编码初始状态$z,s\leftarrow f^{encoder}(x)$；
	3. 初始化管理者和工人的隐藏状态$h^M\leftarrow h^M_0$，$h^W\leftarrow h^W_0$；
	4. 初始化目标队列$G$；
	5. 对每一个时间步循环：
		1. 如果$t\%c=0$：
			1. 管理者决策$g,h^M\leftarrow\mu(s,h^M;\theta^M)$；
			2. 将$g$加入队列$G$，并保持队列长度为$c$（移除旧目标）；
		2. 目标嵌入$w\leftarrow\phi(\sum_{g\in G}g)$；
		3. 生成最终策略$\pi,h^W\leftarrow\pi(z,w,h^W;\theta^W)$；
		4. 根据$\pi$采样动作$a$；
		5. 执行动作$a$，观测奖励$r$和后继状态$x'$；
		6. 编码后继状态$z',s'\leftarrow f^{encoder}(x')$；
		7. 将$(x,a,r,x')$存入缓存区$\mathcal D$；
		8. 更新工人策略参数$\theta^W$；
		9. 更新工人评论家参数$\theta^{V_W}$；
		10. 如果$t+1\%c=0$：
			1. 更新管理者策略参数$\theta^M$；
			2. 更新管理者评论家参数$\theta^{V_M}$；