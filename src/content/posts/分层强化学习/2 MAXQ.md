---
title: 【分层强化学习】#2 MAXQ：值函数分解
published: 2025-11-11
description: 任务分解和值函数分解理论。
tags: [MAXQ]
category: 分层强化学习
---
# MAXQ

**MAXQ**是分层强化学习的奠基性工作之一，它通过任务分解和值函数分解，将复杂的强化学习问题转化为一系列简单的子问题，是在选项框架和半马尔可夫过程的基础上对分层强化学习任务的进一步分析和具体实现。

## 任务分解

我们先以原论文中的问题为例展示如何对一个问题进行任务分解：在一个$5\times5$的网格世界中有R、B、G和Y四个地点，乘客会随机出现在其中一个地点（起点）并随机希望被运送到另一个地点（目的地），作为智能体的出租车则需要先前往起点接上乘客，再前往目的地放下乘客。

![Pasted image 20251110144303|183](https://inkem-1306784622.cos.accelerate.myqcloud.com/blog/pic/Pasted%20image%2020251110144303.png)

出租车可以执行六个原始动作：

- 向东$\mathsf{East}$、向南$\mathsf{South}$、向西$\mathsf{West}$、向北$\mathsf{North}$移动一个方格；
- 接人动作$\mathsf{Pickup}$；
- 放人动作$\mathsf{Putdown}$。

为了构造该问题的MAXQ分解，我们可以在原始动作之上定义以下四个任务：

- $\mathsf{Navigate}(t)$：从当前位置移动到四个地点之一$t$；
- $\mathsf{Get}$：从当前位置移动到乘客的起点并接上乘客；
- $\mathsf{Put}$：从当前位置移动到乘客的目的地并放下乘客；
- $\mathsf{Root}$：完整的出租车任务。

每个任务和原始动作之间的层次结构如下图所示

![Pasted image 20251110145801|500](https://inkem-1306784622.cos.accelerate.myqcloud.com/blog/pic/Pasted%20image%2020251110145801.png)

现在我们来对MAXQ的任务分解进行形式化定义。对于给定的MDP任务$M$，MAXQ将其分解为一组子任务$\{M_0,M_1,\cdots,M_n\}$，并约定$M_0$为根任务，即解决$M_0$等价于解决了原始的MDP任务$M$。

在MAXQ上下文中，策略执行的对象，包括子任务和原始动作，将统一表示为动作$a$。而明确为子任务的对象则会表示为$i$。

## 任务描述

MAXQ将一个非参数化的子任务$M_i$描述为一个三元组$\langle T_i,A_i,\tilde R_i\rangle$：

- $T_i(s)$
	- 终止谓词，将状态空间$\mathcal S$划分为两类，一类是能执行$M_i$的活跃状态$S_i$，另一类是标志着$M_i$已完成的终止状态$T_i$。例如$\mathsf{Get}$的活跃状态为“出租车上没有乘客”，终止状态为“出租车上有乘客”；
	- $M_i$只能在活跃状态下被启动或继续；
	- 终止谓词在选项框架的表述下相当于一个确定性的终止条件$\beta(s)$，即$M_i$在$T_i$下终止的概率为$1$，在$S_i$下终止的概率为$0$。
- $A_i$
	- 可以执行以实现$M_i$的动作集合，其中的动作可以是来自MDP动作空间$\mathcal A$的原始动作或是其他子任务。例如$\mathsf{Get}$的动作集合包括原始动作$\mathsf{Pickup}$和子任务$\mathsf{Navigate}(t)$；
	- 如果其它子任务$M_j$具有形式参数，则不同参数值下的$M_j$对应不同的动作。例如对于不同的$t$，$\mathsf{Navigate}(t)$在$A_i$中是不同的元素；
	- $A_i$实际上是状态$s$的函数$A_i(s)$，因为某些动作不是在所有状态下都可行。例如出租车不能在没有乘客的位置执行$\mathsf{Pickup}$。在MAXQ算法的实际实现中，子任务的可行状态即其活跃状态，而原始动作的可行状态则通过环境奖励学习。
- $\tilde R_i(s'|s,a)$
	- 伪奖励函数，用于评估$M_i$的完成质量，为子策略提供最大化目标，是在环境奖励之外人为设置的；
	- 仅在状态从活跃状态转移到终止状态，即$s\in S_i$和$s'\in T_i$时才触发，告诉智能体$M_i$以这种方式结束的好坏；
	- 例如对于子任务$\mathsf{Put}$，我们可以为其设置“正确放下乘客”“错误放下乘客”“撞墙”“超时”四种终止状态，则只有任务以“正确放下乘客”结束时，我们才给予智能体正奖励，其余情况给予负奖励。

如果子任务$M_i$具有形式参数，则不同参数值下的$M_i$也由不同的三元组描述。实践中我们可以将参数化转移到三元组的元素中，即对于包含形式参数$b$的$M_i$，采用$T_i(s,b)$和$\tilde R_i(s'|s,a,b)$来描述。形式参数在任务抽象的灵活化中起着重要作用，在MAXQ算法的任何步骤中，只要涉及对（具有形式参数的）子任务的选择、执行和评估等操作，相应的结果就必然与参数绑定。

MAXQ将子任务的描述方式推广到了原始动作$a$，但是在定义上相比子任务有所退化：

- 总是可执行的，且执行一步后立刻终止。即活跃状态$S_a$和终止状态$T_a$均包括整个状态空间$\mathcal S$，退化了一分为二的规则；
- 在任何状态下只能执行自己。即动作集合$A_a$只包含原始动作本身$\{a\}$，退化了不能以所描述的子任务对象为元素的规则；
- 伪奖励函数$\tilde R_a(s'|s,a)=0$。

## 分层策略执行

分层策略$\pi$是问题中子任务策略的集合$\pi=\{\pi_0,\cdots,\pi_n\}$，每个子策略$\pi_i$都对应一个子任务$M_i$。子策略$\pi_i$获取状态并返回要执行的动作。

MAXQ的分层策略执行机制是基于栈的，其关键在于将高层任务不断细化，直到分解为可执行的原始动作，并由栈保存任务的执行上下文和参数。其具体流程如下：

1. 在时间步$t$，有
	1. 环境状态$s_t$；
	2. 执行栈状态$K_t$。
2. 如果$K_t$的栈顶不是一个原始动作，则循环
	1. 查询$K_t$的栈顶，获取当前需要执行的子任务$i$及其参数$f_i$；
	2. 查询子任务$i$的策略$\pi_i$，$\pi_i$根据当前的状态$s$和参数$f_i$给出要执行的动作$a$及其参数$f_a$；
	3. 将决策结果$(a,f_a)$压入$K_t$的栈顶。
3. 弹出$K_t$的栈顶$(a,f_a)$。此时$a$一定是一个原始动作，原始动作不具有参数，用$f_a=nil$表示；
4. 执行动作$a$，环境状态由$s_t$更新为$s_{t+1}$；
5. 如果$s_{t+1}$是$K_t$的栈顶任务的终止状态，则循环
	1. 弹出$K_t$的栈顶$(i,f_i)$。
6. 执行栈状态由$K_t$更新为$K_{t+1}$。

下图演示了出租车问题的分层策略执行过程中执行栈的状态变化

![Pasted image 20251111154858](https://inkem-1306784622.cos.accelerate.myqcloud.com/blog/pic/Pasted%20image%2020251111154858.png)

## 投影值函数

为了将值函数推广到任意层级的子任务或原始动作中，MAXQ定义了投影状态价值函数$V^\pi(a,s)$，其中$a$是策略执行的动作，$s$是当前状态。投影值函数将值函数的计算限制在了由动作$a$决定的范围内。

如果动作$a$是原始动作，则$V^\pi(a,s)$就是在状态$s$执行原始动作$a$的期望即时收益

$$
V^\pi(a,s)=\mathbb E[r(s,a)]
$$

如果动作是子任务$i$，则$V^\pi(i,s)$表示从状态$s$开始，遵循分层策略$\pi$（中针对任务$i$的子策略$\pi_i$），直到进入子任务$i$的终止状态集（即任务$i$完成）所能获得的期望累积奖励

$$
\begin{split}
V^\pi(i,s)&=\mathbb E_\pi\left[\left.\sum^{N_i-1}_{t=0}\gamma^tr_t\right|s_0=s\right]\\
\text{（贝尔曼方程）}&=\mathbb E_{a\sim\pi_i(s)}[V^\pi(a,s)+\mathbb E_{s',N_a\sim P^\pi_i(\cdot|s,a)}[\gamma^{N_a}V^\pi(i,s')]]
\end{split}
$$

其中：

- $N_i$为从状态$s$开始，到达子任务$i$的一个终止状态所经历的步数；
- $N_a$
	- 当$a$为子任务时，与$N_i$同理；
	- 当$a$为原始动作时，$N_a=1$。
- $s'$为执行完动作$a$后达到的状态。

和传统强化学习不同的是，在分层强化学习的设计下，一个子任务的V值中即时收益的计算依赖于动作$a$的V值，而折扣指数$N_a$和后继状态$s'$也依赖于动作$a$的执行。这使得V值的学习需要一个复杂的递归过程，即最底层的原始动作直接从环境奖励中学习V值，而高层的子任务则从底层先学习到的V值中学习。

相应地，MAXQ还定义了投影动作价值函数（投影Q函数）$Q^\pi(i,s,a)$，即子任务$i$在状态$s$中执行动作$a$，然后遵循分层策略$\pi$，直到子任务$i$终止所能获得的期望累积奖励

$$
\begin{split}
Q^\pi(i,s,a)&=V^\pi(a,s)+\mathbb E_{s',N_a\sim P^\pi_i(\cdot|s,a)}[\gamma^{N_a}V^\pi(i,s')]\\
\text{（贝尔曼方程）}&=V^\pi(a,s)+\mathbb E_{s',N_a\sim P^\pi_i(\cdot|s,a),a'\sim\pi(s')}[\gamma^{N_a}Q^\pi(i,s',a')]
\end{split}
$$

将其与$V^\pi(i,s)$联系可知，$V^\pi(i,s)$即$Q^\pi(i,s,a)$在$a\sim\pi_i$分布下的期望。

$Q^\pi(i,s',a')$中$a'\sim\pi(s')$而非$\pi_i(s')$的原因是，后继状态$s'$可能会涉及子任务$i$的完成，需要重新基于在状态$s'$下活跃的子任务进行决策。具体从哪一个层级的子任务决策则由执行栈决定，在数学公式表达能力的限制下只能记为$a'\sim\pi(s')$。

## 值函数分解

值函数分解便是基于投影Q函数的贝尔曼方程给出的，我们定义方程中的期望部分为完成函数$C^\pi(i,s,a)$，表示从状态$s$执行动作$a$之后开始，直到完成子任务$i$的期望累积奖励

$$
C^\pi(i,s,a)=\mathbb E_{s',N_a\sim P^\pi_i(\cdot|s,a),a'\sim\pi(s')}[\gamma^{N_a}Q^\pi(i,s',a')]
$$

则投影Q函数可以表示为

$$
Q^\pi(i,s,a)=V^\pi(a,s)+C^\pi(i,s,a)
$$

由此我们也可以进一步写出完成函数的贝尔曼方程

$$
C^\pi(i,s,a)=\mathbb E_{s',N_a\sim P^\pi_i(\cdot|s,a),a'\sim\pi(s')}[\gamma^{N_a}(V^\pi(a',s')+C^\pi(i,s',a'))]
$$

现在，我们建立起了子任务$i$与其下一层子任务$a$，也就是一对父子任务之间的值函数递归关系。可以简单理解为，父任务$i$一方面关心子任务$a$本身的执行过程带来的收益$V^\pi(a,s)$，另一方面关心子任务$a$的完成所带来的新局面$s'$的价值$C^\pi(i,s,a)$。

## MAXQ图

MAXQ图用于可视化值函数分解的递归关系设计

![Pasted image 20251110235122|500](https://inkem-1306784622.cos.accelerate.myqcloud.com/blog/pic/Pasted%20image%2020251110235122.png)

其中三角形的Max节点$i$对应任务分解中的子任务（无Max前缀的为原始Max节点），存储$V^\pi(i,s)$；圆角矩形的Q节点对应每个子任务可用的动作，每个Q节点是父任务$i$和子任务$a$的一一对应，存储$C^\pi(i,s,a)$。

除了静态存储价值函数，这些节点还主动参与计算。Max节点$i$计算$V^\pi(i,s)$，它会询问所有对应的$Q$节点得到它们的$Q^\pi(i,s,a)$，再根据自己的策略$\pi_i$对其加权求和；Q节点（父任务$i$，子任务$a$）计算$Q^\pi(i,s,a)$，它会询问子任务Max节点$a$得到它的$V^\pi(a,s)$，再与自己存储的$C^\pi(i,s,a)$相加，而$C^\pi(i,s,a)$则是通过学习得到的。

最终，根任务的投影状态价值函数的计算会展开如下（第一层分解）

$$
V^\pi(0,s)=\mathbb E_{(a_1,a_2,\cdots,a_m)\sim\mathrm{Traj(\pi|s)}}[V^\pi(a_m,s)+C^\pi(a_{m-1},s,a_m)+\cdots+C^\pi(a_1,s,a_2)+C^\pi(0,s,a_1)]
$$

上式等价于传统RL中的期望回报计算。

![Pasted image 20251111110735|500](https://inkem-1306784622.cos.accelerate.myqcloud.com/blog/pic/Pasted%20image%2020251111110735.png)

## MAXQ-Q

MAXQ-Q是基于MAXQ值函数分解的Q学习算法，其目的是学习最优投影状态值函数$V(i,s)$，在此过程中也会学习最优完成函数$C(i,s,a)$。

对于原始Max节点$i$，$V(i,s)$的更新目标是期望即时收益$\mathbb E[r(i,s)]$

$$
V(i,s)\leftarrow V(i,s)-\alpha(V(i,s)-r)
$$

对于高层Max节点$i$，$V(i,s)$则直接由执行动作的$V(a,s)$和最优完成函数$C(i,s,a)$计算。

我们需要使用贝尔曼最优方程来学习最优完成函数，这涉及对后继状态$s'$选择贪心动作$a^*$。由于$a^*$的目标是更好地完成子任务，因此需要引入伪奖励函数来评估。MAXQ-Q通过学习一个含伪奖励的最优完成函数$\tilde C(i,s,a)$来选择$a^*$

$$
\tilde C(i,s,a)\leftarrow\tilde C(i,s,a)-\alpha[\tilde C(i,s,a)-\gamma^{N_a}(\tilde R_i(s')+\tilde C(i,s',a^*)+V(a^*,s))]
$$

同时定义$\tilde Q(i,s',a')$为含伪奖励的投影Q函数，则贪心动作为

$$
\begin{split}
a^*&=\underset{a'}{\mathrm{argmax}}[\tilde Q(i,s',a')]\\
&=\underset{a'}{\mathrm{argmax}}[\tilde C(i,s',a')+V(a',s')]
\end{split}
$$

我们同时学习原始的最优完成函数用于计算高层Max节点$i$的$V(i,s)$

$$
C(i,s,a)\leftarrow C(i,s,a)-\alpha[C(i,s,a)-\gamma^{N_a}(C(i,s',a')+V(a',s))]
$$

MAXQ图的Q节点也将存储$\tilde C(i,s,a)$和计算$\tilde Q(i,s',a')$。

MAXQ-Q算法通过MAXQ-Q函数的递归执行实现，该函数接收Max节点$i$和状态$s$作为输入，返回执行$i$时访问的状态序列$seq$。下面是MAXQ-Q函数的伪代码：

1. 输入Max节点$i$，状态$s$；
2. 初始化$seq$为执行$i$时访问的状态序列（按时间步顺序从后往前）；
3. 如果$i$为原始Max节点
	1. 执行$i$，接收即时收益$r$和后继状态$s'$；
	2. 使用TD方法更新$V_{t+1}(i,s)$；
	3. 将$s$加入$seq$的开头。
4. 否则
	1. 当$T_i(s)=False$（任务未终止），循环
		1. 根据当前探索策略$\pi_x(i,s)$选择动作$a$；
		2. 初始化$childSeq=\text{MAXQ-Q(a,s)}$为执行$a$时访问的状态序列；
		3. 获取完成$a$的后继状态$s'$；
		4. 选择贪心动作$a^*=\underset{a'}{\mathrm{argmax}}[\tilde Q(i,s',a')]$；
		5. 获取$a$的执行步数$N_a=\mathrm{length}(childSeq)$
		6. 依次对$childSeq$中的每个状态$s$
			1. 更新最优完成函数$C(i,s,a)$和含伪奖励的最优完成函数$\tilde C(i,s,a)$；
			2. $N_a\leftarrow N_a-1$。
		7. 将$childSeq$加入$seq$的开头；
		8. $s\leftarrow s'$。
5. 返回$seq$。

完成函数$C(i,s,a)$遍历了执行动作$a$所经历的所有状态而非初始状态，可以最大限度地利用完成一个子任务产生的经验，来改进所有中间状态的决策价值。

最终，MAXQ-Q将会收敛于唯一的最优分层策略$\pi^*$，它是每个子任务的最优子策略$\pi^*_i$的集合，最优子策略$\pi^*_i$对含伪奖励的最优投影Q函数$\tilde Q^*(i,s,a)$贪心。