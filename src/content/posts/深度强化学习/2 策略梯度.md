---
title: 【深度强化学习】#2 策略梯度定理
published: 2025-10-08
description: 基于策略的方法的理论基础。
tags: [策略梯度, REINFORCE]
category: 深度强化学习
---
# 前言

到目前为止，本系列所介绍的所有强化学习方法几乎都是**基于价值的方法**，它们都是先学习动作价值函数，然后根据估计的动作价值函数选择动作，没有动作价值函数的估计就没有策略。

**策略梯度**（PG，Policy Gradient）是一种**基于策略的方法**，它直接学习策略函数，通过策略函数直接输出动作。价值函数仍然可以用于策略函数的学习，但对于动作的选择而言就不是必须的了。

# 策略函数

和价值函数类似，**策略函数**不再使用表格$\pi(a|s)$而是一个线性函数$\pi(a|s,\theta)$来表示策略，其输入是状态，输出是所有动作的概率，因而适合于无穷大的状态空间。又因为策略函数作为概率分布可以是连续的，因而也适合于连续的动作空间。

但是神经网络的原始输出是任意的实数值，有正有负且和不为$1$，不是一个合法的概率分布，为此需要softmax函数映射

$$
\pi(a|s,\theta)=\frac{e^{h(s,a,\theta)}}{\displaystyle\sum_{a'\in\mathcal A}e^{h(s,a',\theta)}}
$$

其中$h(s,a,\theta)$是由神经网络表示的函数。

有了策略函数，策略梯度的基本思路即

1. 寻找一个目标函数$J(\theta)$来定义最优策略。
2. 基于梯度优化寻找最优策略：$\theta_{t+1}=\theta_{t}+\alpha\nabla_\theta J(\theta_t)$

# 目标函数

策略梯度具有多种多样的目标函数表达形式，为了更好地入门，本篇只介绍其中最直接最基础的目标，即期望回报最大化。设$\tau$为智能体与环境交互产生的轨迹$(s_0,a_0,r_0,s_1,a_1,r_1,\cdots,s_T,a_T,r_T)$，$R(\tau)$为轨迹的累积奖励$\displaystyle\sum^T_{t=0}\gamma^tr_t$，$\gamma$为折扣因子，则对应的目标函数即

$$
J(\theta)=\mathbb E_{\tau\sim\pi_\theta}[R(\tau)]=\sum_\tau P(\tau|\theta)R(\tau)
$$

其中$P(\tau|\theta)$表示在策略$\pi_\theta$下轨迹$\tau$出现的概率，将其展开可以得到

$$
\begin{split}
P(\tau|\theta)&=p(s_0)\pi_\theta(a_0|s_0)p(s_1|s_0,a_0)\pi_\theta(a_1|s_1)p(s_2|s_1,a_1)\cdots\\
&=p(s_0)\prod^T_{t=0}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)
\end{split}
$$

现在我们来求目标函数的梯度

$$
\nabla_\theta J(\theta)=\sum_\tau R(\tau)\nabla_\theta P(\tau|\theta)
$$

对于$P(\tau|\theta)$累乘项可以利用对数导数技巧$f'(x)=f(x)(\ln f(x))'$，得

$$
\begin{split}
\nabla_\theta J(\theta)&=\sum_\tau R(\tau)P(\tau|\theta)\nabla_\theta \ln P(\tau|\theta)\\
&=\mathbb E_{\tau\sim\pi_\theta}[\nabla_\theta\ln P(\tau|\theta)\cdot R(\tau)]
\end{split}
$$

其中对数项的梯度计算如下

$$
\begin{split}
\ln P(\tau|\theta)&=\ln p(s_0)+\sum^T_{t=0}\ln\pi_\theta(a_t|s_t)+\sum^T_{t=0}\ln p(s_{t+1}|s_t,a_t)\\
\nabla_\theta\ln P(\tau|\theta)&=\sum^T_{t=0}\nabla_\theta\ln\pi_\theta(a_t|s_t)
\end{split}
$$

来自环境的$p$与$\theta$无关，因此梯度为$0$，而策略函数的对数求导由深度学习中的自动微分完成。合并上述结果，策略梯度最终为

$$
\nabla_\theta J(\theta)=\mathbb E_{\tau\sim\pi_\theta}\left[\left(\sum^T_{t=0}\nabla_\theta\ln\pi_\theta(a_t|s_t)\right)\cdot R(\tau)\right]
$$

# 梯度上升

梯度上升算法用于最大化目标函数，其基本思路为

$$
\begin{split}
\theta_{t+1}&=\theta_t+\alpha\nabla_\theta J(\theta)\\
&=\theta_t+\alpha\mathbb E_{\tau\sim\pi_\theta}\left[\left(\sum^T_{t=0}\nabla_\theta\ln\pi_\theta(a_t|s_t)\right)\cdot R(\tau)\right]
\end{split}
$$

但是大多数情况下我们不知道精确的环境模型，无法确定期望中的概率分布，因此只能使用随机梯度来代替这个真实梯度

$$
\theta_{t+1}=\theta_t+\alpha\left(\sum^T_{t=0}\nabla_\theta\ln\pi_\theta(a_t|s_t)\right)\cdot R(\tau)
$$

这个更新仍然不够高效，因为每次更新前我们都需要采样一条完整的轨迹。如果将上式的求和符号拆开，我们可以发现使用一条轨迹进行一次更新相当于对轨迹中的每一个状态动作对都进行一次单独的更新

$$
\theta_{t+1}=\theta_t+\alpha\nabla_\theta\ln\pi_\theta(a_t|s_t)\cdot R(\tau)
$$

除此之外，尽管该更新公式中对于$\nabla_\theta J(\theta)$的估计是无偏的，但它具有较高的方差，进而导致收敛缓慢。这是因为与每个动作相关的梯度计算都被整个轨迹的回报$R(\tau)$缩放，对于越靠后的动作，它将包含越多来自过去的奖励的噪声，这也不符合动作只对未来奖励负责的因果关系。将整个轨迹的回报$R(\tau)$替换为动作$a_t$之后的回报$G_t(\tau)$（也可以等价为其估计目标动作价值函数$q_{\pi_\theta}(s_t,a_t)$），能够在不改变估计的无偏性的基础上降低其方差

$$
\begin{split}
\theta_{t+1}&=\theta_t+\alpha\nabla_\theta\ln\pi_\theta(a_t|s_t)\cdot G_t(\tau)\\
&=\theta_t+\alpha\nabla_\theta\ln\pi_\theta(a_t|s_t)\cdot q_{\pi_\theta}(s_t,a_t)
\end{split}
$$

如果将对数求导技巧还原，我们可以更好理解这个梯度上升的意义

$$
\begin{split}
\theta_{t+1}&=\theta_t+\alpha\nabla_\theta\ln\pi_\theta(a_t|s_t)\cdot q_{\pi_\theta}(s_t,a_t)\\
&=\theta_t+\alpha\underset{\beta_t}{\underbrace{\frac{q_{\pi_\theta}(s_t,a_t)}{\pi_\theta(a_t|s_t)}}}\nabla_\theta\pi_\theta(a_t|s_t)
\end{split}
$$

分析其中的$\beta_t$，可知

- 更大的动作价值会提升相应动作的选择概率。
- 概率较小的动作会从上一点中得到更大幅度的更新。

因此策略梯度很好地平衡了试探与开发的问题。

# REINFORCE及其改进

上述公式给出了一个显然的采样估计方法，即

1. 用当前策略$\pi_\theta$与环境交互，收集一条完整轨迹$\tau=(s_0,a_0,r_0,\cdots,s_T,a_T,r_T)$
2. 对该条轨迹计算每个时刻$t$到结束的累积奖励$G_t=\displaystyle\sum^T_{k=t}\gamma^{k-t}r_k$
3. 对每个时刻$t$，更新参数$\theta_{t+1}=\theta_t+\alpha\nabla_\theta\ln\pi_\theta(a_t|s_t)\cdot G_t(\tau)$
4. 完成所有时刻的更新任务后，将参数$\theta$的最终更新结果应用到下一轮轨迹采样中。

这种使用蒙特卡洛方法来得到估计目标$q_{\pi_\theta}(s_t,a_t)$的算法被称为**REINFORCE**，即蒙特卡洛策略梯度，它是最早最简单的策略梯度算法。

但是，仅凭一次轨迹采样来完成一轮更新仍然会带来一定的随机性和方差，更进一步加快收敛的方法是引入**基线函数**$b(s)$。带基线的策略梯度公式为

$$
\theta_{t+1}=\theta_t+\alpha\nabla_\theta\ln\pi_\theta(a_t|s_t)\cdot [G_t(\tau)-b(s)]
$$

这并不影响其无偏性，因为

$$
\begin{split}
&\mathbb E_{a\sim\pi_\theta}[\nabla_\theta\ln\pi_\theta(a|s)\cdot b(s)]\\
=&b(s)\mathbb E_{\tau\sim\pi_\theta}[\nabla_\theta\ln\pi_\theta(a|s)]\\
=&b(s)\int\pi_\theta(a|s)\nabla_\theta\ln\pi_\theta(a|s)\mathrm da\\
=&b(s)\int\pi_\theta(a|s)\frac{\nabla_\theta\pi_\theta(a|s)}{\pi_\theta(a|s)}\mathrm da\\
=&b(s)\int\nabla_\theta\pi_\theta(a|s)\mathrm da\\
=&b(s)\nabla_\theta\int\pi_\theta(a|s)\mathrm da\\
=&b(s)\nabla_\theta (1)\\
=&0
\end{split}
$$

在某个状态$s$，所有动作的回报$G_t$可能都很高，这将导致较大且方向相似的更新幅度。减去一个与动作无关的基线$b(s)$可以在显著降低方差的同时凸显不同动作之间的相对差异，这也更符合策略函数在比较中选择的实际意义。

理论上，任何只与状态$s$有关的函数都可以作为基线，但最常用的基线是状态价值函数$V_{\pi_\theta}(s)$，它具有良好的解释性：$G_t(\tau)$是动作$a$在状态$s$下的样本实际回报，$V_{\pi_\theta}(s)$是状态$s$下所有动作的期望回报，它们的差值$G_t(\tau)-V_{\pi_\theta}(s)$正好是**优势函数**$A_{\pi_\theta}(s,a)$的蒙特卡洛估计，它衡量了动作$a$相对于平均水平的优势。这也将自然引出了我们下一篇要介绍的Actor-Critic方法。