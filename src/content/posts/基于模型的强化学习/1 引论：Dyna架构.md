---
title: 【基于模型的强化学习】#1 引论：Dyna架构
published: 2025-12-17
description: 主动学习环境动力学模型的方法。
tags: [Dyna, 引论]
category: 基于模型的强化学习
---
# 基于模型的强化学习

环境的**模型**（Model）是一个智能体可以用来预测环境对其动作的反应的任何事物。给定一个状态和一个动作，模型能产生后继状态和下一个收益的预测作为环境的反应结果。根据是否有模型参与，强化学习算法可分为**基于模型的强化学习**（Model-Based RL，MBRL）和**无模型强化学习**（Model-Free RL，MFRL）。

在【深度强化学习】和【分层强化学习】中，我们探讨的都是MFRL，它们将环境视为一个黑箱，智能体直接从与环境交互获得的经验数据中学习价值函数或策略，而不会尝试理解环境的内部运作机制。这种黑箱式的学习往往存在样本效率低下的瓶颈，需要与环境进行大量的交互才能掌握一项复杂任务，在现实世界中更会带来庞大的训练成本和极高的安全隐患。即使有仿真技术的加持，其对于复杂环境也需要巨大的专家知识和工程投入，并且不可避免地会与现实存在偏差（现实鸿沟）。

基于上述缺陷，MBRL提出智能体应该主动学习一个环境动力学模型作为其内部的、可改进的“仿真器”以辅助决策。拥有环境模型作为信息补充，MBRL相较于MFRL可以有如下优势：

- **样本效率**：能够利用环境模型进行内部规划或想象，减少对真实经验的依赖；
- **泛化能力**：环境模型的学习可以从见过的状态推理泛化到未见过的状态，并可以通过在线学习快速微调适应新环境或弥补现实鸿沟；
- **探索效率**：能够根据环境模型判断状态的探索优先级，从而在复杂状态空间中进行有效的探索。

在MBRL中，模型的使用方式主要有以下两种：其一是用于**数据增强**（Data Augmentation），即利用内部模型合成的数据来扩增策略的训练数据，提升策略的训练效率，该用途可以与MFRL相结合；其二是用于**规划**（Planning），即在决策阶段，智能体先使用模型进行多步推演（rollout），选择最优动作序列执行其第一步，不依赖显式的策略网络，而是实时思考。

需要注意的是，MBRL和MFRL并非对立，MBRL大多是以模型为核心、内嵌MFRL更新的混合架构，只要显式学习并利用了模型，算法就归属于MBRL范式。

# Dyna架构

**Dyna架构**（1990）是基于表格型方法被提出的经典MBRL方法，对现代MBRL产生了深远影响。它将环境模型无缝引入MFRL，交替使用模型采样和环境采样，使得二者的优缺点得到了良好的互补。

Dyna的核心思想是，对一个智能体来说，与环境交互得到的经验可以扮演两个角色，它能用来改进模型，或是直接更新价值函数和策略。前者称为**模型学习**，后者则称为**直接强化学习**。改进后的模型则通过规划影响价值函数和策略，这种与规划相关的方式称为**间接强化学习**。直接强化学习类似于试错学习和被动的反应式决策，而间接强化学习则代表了原理认知和主动的预谋性规划。下图展示了经验、模型、价值和策略之间的关系：

![Pasted image 20251217204511|350](https://inkem-1306784622.cos.accelerate.myqcloud.com/blog/pic/Pasted%20image%2020251217204511.png)

Dyna架构的提出者Sutton在《强化学习（第二版）》介绍该方法时虽然用“规划”来描述模型的作用，但根据博主研究，Dyna架构下的规划并非前文提及的现代MBRL下完全意义的规划，其核心仍是利用模型作为数据生成器为价值函数和策略的更新提供额外的虚拟经验。

## Dyna-Q

Dyna-Q是Dyna架构的一个具体的经典实现，其通过Q学习进行直接强化学习更新和规划更新。

Dyna-Q的模型学习方法也是基于表格的，并且假设环境是确定的。在每次转移$S_t,A_t\rightarrow R_{t+1},S_{t+1}$之后，模型在它的表格中会为$S_t,A_t$建立条目，记录环境在这种情况下产生的转移结果的预测值$R_{t+1},S_{t+1}$。规划过程中，算法将随机从模型之前学习到的“状态-动作”二元组进行采样并返回历史观测值。

![Pasted image 20250521123010|475](https://inkem-1306784622.cos.accelerate.myqcloud.com/blog/pic/Pasted%20image%2020250521123010.png)

从概念上讲，规划、动作执行、模型学习和直接强化学习在Dyna的智能体中是并行进行的，但在串行计算机中我们需要指定它们的发生顺序。动作执行、模型学习和直接强化学习过程只需要很少的计算，而规划则需要计算密集的迭代过程，因此我们在每次循环中先执行前者，最后将剩余的计算用于规划。根据这一思想，我们给出Dyna-Q的算法流程。令$Model(s,a)$基于“状态-动作”二元组$(s,a)$预测后继状态和收益，$policy(S,Q)$是状态$S$下针对$Q$的$\epsilon$-贪心策略，更新Q值的方式为

$$
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha[R_{t+1}+\gamma\underset a\max Q(S_{t+1},a)-Q(S_t,A_t)]
$$

则表格型Dyna-Q算法的伪代码如下：

1. 对所有的$s\in\mathcal S$和$a\in\mathcal A(s)$，初始化$Q(s,a)$和$Model(s,a)$
2. 循环：
	1. $S\leftarrow$当前（非终止）状态；
	2. $A\leftarrow policy(S,Q)$；
	3. 采取动作$A$，观察产生的收益$R$和状态$S'$；
	4. 根据$(S,A,R,S')$更新Q值；
	5. $Model(S,A)\leftarrow R,S'$（假设环境是确定的）；
	6. 重复$n$次循环：
		1. $S\leftarrow$随机选择之前观察到的状态；
		2. $A\leftarrow$随机选择之前在状态$S$下采取过的动作$A$；
		3. $R,S'\leftarrow Model(S,A)$；
		4. 根据$(S,A,R,S')$更新Q值。

## Dyna-Q+

Dyna-Q假设环境是确定的，此时模型总是被填充完全正确的信息。但在更多时候我们没有这么幸运，以下这些原因都会导致模型错误：

- 环境是随机的，且只有数量有限的样本会被观察到；
- 模型用于近似环境的函数泛化能力较差；
- 环境发生改变且新的动态特性尚未被观察到。

当模型错误时，规划过程就可能计算出次优的策略。

在某些情况下，规划计算出的次优策略能使得Dyna-Q很快发现并修正模型错误，例如环境变化导致原始最优策略的回报大大降低，模型能很快沿着策略所在路径感知到自身信息和环境不匹配，进而修正错误。但如果这种变化对原始最优策略没有影响，而是产生了原始最优策略之外的捷径，那么Dyna-Q将很难摆脱对原始最优策略的依赖去探索更优的策略。

Dyna-Q+在Dyna-Q的基础上采用启发式方法，在一定程度上克服了这种问题。Dyna-Q+的智能体会对每一个“状态-动作”二元组进行跟踪，记录它自上一次在与环境进行真实交互以来已经过了多少时刻。时间越长，我们就越有理由推测这个二元组相关的环境动态特性会产生变化，也即关于它的模型是不正确的。

为了鼓励测试长期未出现过的“状态-动作”二元组，一个和未出现时间相关的“额外收益”将会提供给智能体。如果模型记录的单步转移收益为$r$，而这个转移在$\tau$时刻内没有尝试，那么在更新时就会采用$r+\kappa\sqrt\tau$的收益，其中$\kappa$是一个比较小的正数。这会鼓励智能体不断试探所有可访问的状态转移，甚至使用一长串的动作完成这种试探。

Dyna架构虽受限于其表格形式和确定性假设，但其中“虚实结合、模型辅助决策”的核心思想，却直接启发了现代MBRL的发展。如今，随着深度表示学习、概率建模与高效规划算法的进步，现代MBRL方法已能在高维连续空间中学习复杂的环境动力学模型，实现更高效的样本利用、更鲁棒的策略学习以及更自主的想象式规划，将在后续逐步介绍。

# 系列前瞻

本系列后续算法更新规划如下（暂定）：

1. PETS：带有轨迹采样的概率集成模型
2. MBPO：基于模型的策略优化
3. PlaNet：潜在空间深度规划网络
4. MuZero：现代树搜索

【世界模型】作为【基于模型的强化学习】的一个高级主题，将在其后作为一个子系列更新，这方面尚待调研，大概率包含的算法如下：

1. 引论：2018年《World Models》论文
2. DreamerV1
3. DreamerV2
4. DreamerV3
5. SVG（Stochastic Video Generation）
6. Diffusion Policy / Decision Diffuser