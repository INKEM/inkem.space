---
title: 【基于模型的强化学习】#2 PETS：从确定性模型到不确定性模型
published: 2026-02-01
description: 量化并尊重模型的不确定性
tags: [PETS, 概率性神经网络, 集成, 轨迹采样, 模型预测控制, 交叉熵方法]
category: 基于模型的强化学习
---
# PETS

在上一篇中，Dyna架构为MBRL提供了一个优雅的想象学习范式。理论上，只要有一个完美的环境模型，智能体就能在脑海中无限试错，实现极高的样本效率。随着深度学习的发展，神经网络无疑成为了学习环境模型的极佳框架，但是人们发现，神经网络总是无法学到完美的环境模型，在想象中表现优异的策略到真实环境中面临着彻底失效的可能，使得MBRL在渐近性上一直逊色于MFRL。

在Dyna架构提出后的近30年中，MBRL的发展致力于解决如何安全地利用不完美的模型进行规划，但经历的一系列尝试要么假设过强（局部线性模型），要么计算复杂（贝叶斯神经网络）。

直到2018年，**带有轨迹采样的概率集成模型**（Probabilistic Ensembles with Trajectory Sampling，PETS）的出现，才为这一挑战带来了突破性进展。PETS的核心思想在于量化和尊重模型的不确定性，它提出模型的不确定性可以被解耦为两个关键部分：

- **认知不确定性**（Epistemic uncertainty）：因训练数据不足导致的对模型本身的不确定性。
- **偶然不确定性**（Aleatoric uncertainty）：环境动态特性中固有的随机性。

通过概率性神经网络（P）、模型集成（E）和轨迹采样（TS）三大模块，PETS有效地利用了这两种不确定性进行规划，让MBRL从理论优雅走向了实用高效。

## 概率性神经网络

PETS使用**概率性神经网络**（Probabilistic neural networks）作为动力学模型的基础结构来应对源于环境的偶然不确定性。模型接收状态-动作对$(s,a)$作为输入后，一般的神经网络直接输出确定的后继状态$s'$作为预测结果，而概率性神经网络则输出关于$s'$的参数化概率分布函数。

连续状态空间下的概率分布通常采用高斯分布，其假设后继状态的分布只有一个峰值。此时神经网络输出高斯分布的均值$\mu_\theta(s,a)$和方差$\Sigma_\theta(s,a)$，即

$$
\tilde f_\theta=\mathrm{Pr}(s'|s,a)=\mathcal N(\mu_\theta(s,a),\Sigma_\theta(s,a))
$$

损失函数采用负对数预测概率

$$
\begin{split}
\mathrm{loss_{Gauss}}(\theta)&=-\sum^N_{n=1}\log\tilde f_\theta(s_{n+1}|s_n,a_n)\\
&=\sum^N_{n=1}[\mu_\theta(s_n,a_n)-s_{n+1}]^T\Sigma^{-1}_\theta(s_n,a_n)[\mu_\theta(s_n,a_n)-s_{n+1}]+\log\det\Sigma_\theta(s_n,a_n)
\end{split}
$$

为了让采样过程可微分，以便通过梯度下降优化神经网络，后继状态$s'$的采样利用**重参数化技巧**（Reparameterization trick），即不直接从$\mathcal N(\mu_\theta(s,a),\Sigma_\theta(s,a))$中采样$s'$，而是先从一个固定的、与参数$\theta$无关的基础分布（如标准正态分布$\mathcal N(0,1)$）中采样一个随机噪声$\epsilon$，再通过一个确定的、可微的变换构造出$s'$

$$
\begin{split}
s'=\tilde f_\theta(s'|\epsilon;s,a)=\mu_\theta(s,a)+\Sigma_\theta(s,a)\cdot\epsilon\\
\quad\epsilon\sim\mathcal N(0,1)
\end{split}
$$

## 认知不确定性

神经网络的泛化能力是有限的，对于OOD（Out-Of-Distribution，训练数据分布之外的）输入，神经网络的输出将会缺乏可靠性，这便是模型的认知不确定性。而这个问题在MBRL中尤为致命，因为

- 智能体必须探索未知区域以找到更好的策略；
- 利用模型进行规划时，累积预测误差将随着决策多步展开而指数增长；
- 相较于传统监督学习的分类和回归，强化学习面临的许多任务风险更不可控。

在概率性神经网络的输出中，均值决定模型对未来的预测，方差决定模型预测的可靠程度，规划器会综合二者的信息给出决策依据。在OOD输入下，二者的错误会产生不同的危害

- **均值错误**：如果方差合理，规划器会提供更加稳健的决策，并仍有通过探索进行修正的机会。
- **方差错误**
	- 低估：规划器盲目信任模型预测，给出没有依据的确定路径，现实中大概率崩溃；
	- 高估：规划器不相信任何模型预测，智能体规避探索，安全但可能得到次优策略，且无法收集新的经验修正。

由此可见，方差错误相比均值错误具有更高的处理优先级，也是PETS主要解决的问题。

### 方差约束

PETS首先对OOD输入的方差输出进行约束，以解决方差输出的无界性并缓解其不平滑性。一个合理的约束范围是训练数据方差的最大值和最小值之间。

*训练数据方差是概率性神经网络在以训练数据为输入时，从观测到的后继状态中学习到的方差输出。*

为了让梯度信息在约束后得以保留，PETS采用平滑的**softplus约束**而非硬截断，在算法中以对数形式进行运算

$$
\ln\Sigma'=\left\{\begin{matrix}
\ln\Sigma_{\max}-\mathrm{softplus}(\ln\Sigma_{\max}-\ln\Sigma)&(\Sigma>\Sigma_{\max})\\
\ln\Sigma_{\min}+\mathrm{softplus}(\ln\Sigma-\ln\Sigma_{\min})&(\Sigma<\Sigma_{\min})
\end{matrix}\right.
$$

其中softplus函数为

$$
\mathrm{softplus}(x)=\ln(1+e^x)
$$

softplus函数在$x\rightarrow-\infty$时近似为$e^x$（接近$0$），在$x\leftarrow+\infty$时近似为$x$，因此它是对ReLU函数${\max}(0,x)$的平滑近似，而softplus约束运算则是对硬截断操作的平滑近似。

$\Sigma_{\max}$和$\Sigma_{\min}$在模型训练过程中会收敛到训练数据的实际方差边界，但为了有效约束OOD输入的方差，PETS会通过正则化技巧在原有基础上进一步缩小约束范围。概率性神经网络最终的损失函数是负对数似然损失加上两个对$\Sigma_{\max}$过大和$\Sigma_{\min}$过小加以惩罚的正则化项

$$
L_{\mathrm{total}}=L_\mathrm{Gauss}+\lambda_{\max}\ln\Sigma_{\max}-\lambda_{\min}\ln\Sigma_{\min}
$$

其中$\lambda_{\max}$和$\lambda_{\min}$为超参数。

### 集成

方差约束作为一种工程技巧，只能防止方差输出的数值溢出，并提供简单的行为边界，但还是没有从根本上捕捉到认知不确定性。

认知不确定性的本质是模型参数的不确定性。对于ID（In-Distribution，训练数据分布内的）输入，模型的输出将在训练过程中会向确定的方向收敛，而对于OOD输入，模型的输出则会变得不可预测。因此，识别模型认知不确定性的一个切入点在于模型输出的方差，而单一模型的输出无法作为估计方差的依据。

PETS针对这一点给出的解决方案是概率网络**集成**（Essembles），即同时训练$B$个模型，每个模型$\tilde f_{\theta_b}$拥有独立的参数$\theta_b$。为了使模型的训练结果差异化，从而暴露出认知不确定性，每个模型可采用不同的超参数、网络架构、训练数据子集和随机种子。不同模型对同一输入的输出差异越大，认知不确定性越高。这一信息将在规划过程中得到利用。

![](https://inkem-1306784622.cos.accelerate.myqcloud.com/blog/pic/Pasted%20image%2020260201111515.png)

*模型1和模型2分别拟合一个带噪声的三角函数模型，在ID区域二者输出趋同，在OOD区域二者输出明显不一致，且距ID区域越远，分歧越大。*

## 规划

在MBRL中，规划的本质是求解一个有限时域随机最优控制问题：在每个时间步$t$，给定当前状态$s_t$，目标是找到一个动作序列$a_{t:t+H-1}$来最大化期望累积回报

$$
a_{t:t+H-1}=\underset{a_{t:t+H-1}}{\arg{\max}}\mathbb E_{s_{t+k+1}\sim p(\cdot|s_{t+k},a_{t+k})}\left[\sum^{H-1}_{k=0}\gamma^kr(s_{t+k},a_{t+k})\right]
$$

其中$H$为规划视野，$\gamma$为折扣因子，$p(\cdot|s_{t+k},a_{t+k})$由模型给出。

### 交叉熵方法

PETS采用**交叉熵方法**（Cross Entropy Method，CEM）来求解规划问题。CEM是一种基于采样的随机优化方法，其核心思想是用一个参数化的概率分布来表示候选解的空间，在迭代中使其集中于高质量解的区域。

令分布$q_\theta(\tau)$表示动作序列$\tau=a_{t:t+H-1}$的概率，PETS使用时间解耦的高斯分布将其参数化为

$$
q_\theta(\tau)=\prod^{H-1}_{k=0}\mathcal N(a_{t+k}|\mu_k,\Sigma_k)
$$

即每个时间步下候选动作的分布均为高斯分布，且均值$\mu_k$和协方差矩阵$\Sigma_k=\mathrm{diag}(\sigma_k^2)$相互独立，相应地参数为所有时间步均值和方差的二元组$\theta=\{(\mu_k,\sigma_k)\}^{H-1}_{k=0}$。

*为了保证样本效率，我们假设候选动作的高斯分布是各向同性的，即各个动作维度之间无相关性，此时其协方差矩阵为对角元素相同的对角阵。*

设每次采样$K$个候选序列，保留前$M=\lceil\rho K\rceil$个精英样本，则CEM的迭代过程如下：

1. 从当前分布$q_{\theta^{(i)}}$中独立采样$K$个动作序列：$$\tau^{(j)}\sim q_{\theta^{(i)}}(\cdot),\quad j=1,\cdots,K$$
2. 对每个序列计算其期望收益$J(\tau^{(j)})$（计算方法见下一小节）；
3. 将动作序列按收益降序排列，选择前$M$个作为精英样本$\mathcal E$；
4. 用精英样本的统计量更新分布参数：
$$
\mu_k^{(i+1)}=\frac1M\sum_{\tau\in\mathcal E}a_k^{(\tau)},\sigma_k^{(i+1)}=\sqrt{\frac1M\sum_{\tau\in\mathcal E}(a_k^{(\tau)}-\mu_k^{(i+1)})^2}
$$
通常还会使用软更新
$$
\theta^{(i+1)}=\alpha\theta^{(i)}+(1-\alpha)\theta_{\mathrm{new}}
$$
5. 重复上述过程直到达到预设迭代次数或参数收敛。

### 轨迹采样

评估函数$J(\tau)$的计算是PETS规划过程最核心的部分，它必须同时处理认知不确定性和偶然不确定性。实现这一点的关键在于**轨迹采样**（Trajectory Sampling）的设计。

在每个候选序列的每次轨迹展开中：对于认知不确定性，PETS将等概率随机选择集成中的一个模型$f_i\in\{f_1,\cdots,f_B\}$用于预测；对于偶然不确定性，给定模型$f_i$和输入$(s,a)$，模型输出高斯分布参数$(\mu,\Sigma)$，PETS利用重参数化技巧采样后继状态$s'\sim\mathcal N(\mu,\Sigma)$。

PETS将轨迹展开视为一个**状态粒子**$s^p_t$在模型中按照时间步$t$的传播。每个候选序列在单次评估中的轨迹展开次数$P$被称为**粒子数**，一次评估中所有粒子的集合蕴含了未来状态的概率分布。

针对每次随机选择的模型是用于一整条轨迹还是轨迹中的每个时间步，PETS提出了TS∞和TS1两种变体，它们以不同的方式对两种不确定性进行处理。

TS1在一次轨迹展开的每个时间步都重新随机选择一个模型用于预测。对于候选动作序列$\tau=a_{0:H-1}$，一个完整的TS1过程如下：

1. 对从$p=0$到$P$的每个粒子$s^p_0$：
	1. 初始化$s_0^p=s_{\mathrm{current}}$，累积收益$R^p=0$，折扣$d=1$；
	2. 对每个时间步$k=0$到$H-1$：
		1. 随机选择模型索引$i\sim\mathrm{Uniform}\{1,\cdots,B\}$；
		2. 用模型$f_i$预测$(\mu_k,\Sigma_k)=f_i(s_k^p,a_k)$；
		3. 采样后继状态$s_{k+1}^p\sim\mathcal N(\mu_k,\Sigma_k)$；
		4. 计算奖励$r_k=r(s_k^p,a_k)$；
		5. 更新：$R^p\leftarrow R^p+d\cdot r_k$，$d\leftarrow d\cdot\gamma$。
2. $\displaystyle J(\tau)=\frac1P\sum^P_{j=0}R^j$。

TS1在同一条轨迹中混杂运用多个模型，将偶然不确定性和认知不确定性融合处理，这带来了一些根本限制：

- 模型一致性缺失，规划器在物理规律不稳定的状态下可能产生混乱的决策；
- 低估极端情况的风险，模型的融合可能会稀释悲观预测的影响，从而忽视最坏情况；
- 阻碍定向探索，探索算法应当积极探索高认知不确定性的区域而非高偶然不确定性的区域，TS1无法区分二者；
- 策略评估失真，TS1可能会结合乐观预测和悲观预测给出中庸策略，错过更优策略。

PETS主要针对定向探索问题提出了TS∞。TS∞在一次评估中，为每个模型都分配$P/B$个粒子，粒子的传播过程将始终由与之对应的模型预测。TS∞通过保持模型的时间不变性假设，实现了偶然不确定性和认知不确定性的分离。偶然不确定性体现于每个模型\[不同粒子累积回报间的方差\]的均值

$$
\mathrm{Aleatoric}=\frac1B\sum^B_{i=1}\mathrm{var}(R_i^p)^{P/B}_{p=1}
$$

*$R^p_i$表示模型$i$第$p$个粒子的累积回报，方差括号外上下标表示括号内求方差的元素范围。*

认知不确定性则体现于\[不同模型的粒子累积回报均值\]间的方差

$$
\mathrm{Epistemic}=\mathrm{var}\left(\frac1{P/B}\sum^{B/P}_{p=1}R^p_i\right)^B_{i=1}
$$

![](https://inkem-1306784622.cos.accelerate.myqcloud.com/blog/pic/Pasted%20image%2020260201111123.png)

PETS在论文中采用了TS1，因为在短期连续控制任务背景下，TS1的缺陷可以被大量采样和重复规划所缓解，并且计算效率高、实现简单、训练过程稳定。而TS∞则作为一种不确定性分解的理论被提出，为MBRL后续在探索策略设计和更加复杂的RL任务上的发展奠定了思想基础。

PETS没有显式运用定向探索的策略，其探索能力是在规划器的大量采样和期望回报评估中涌现的。除此之外，通过限制CEM的参数$\sigma$的下限，PETS可以始终保持一定程度的动作空间探索。

---

PETS的规划器工作在标准的**模型预测控制**（Model predictive control，MPC）中，即在每个时间步$t$，智能体：

1. 基于当前状态$s_t$，运行CEM优化得到最优动作序列$\tau_t^*$；
2. 执行序列中的第一个动作$a_t=\tau_t^*[0]$；
3. 观测后继状态$s_{t+1}$，重新运行CEM优化。

利用热启动技巧可以显著减少CEM优化的迭代次数，即根据时间连续性，将上一时刻优化得到的序列$\tau^*_{t-1}$向右平移作为初始猜测（均值），最后一个动作重复或置零。

---
![](https://inkem-1306784622.cos.accelerate.myqcloud.com/blog/pic/Pasted%20image%2020260201111317.png)

论文最终给出的PETS算法的一个伪代码如下：

1. 使用一个随机控制器与真实环境进行交互，得到一条轨迹用于初始化数据集$\mathbb D$；
2. 从$k=1$到$K$循环：
	1. 使用$\mathbb D$训练一个概率集成模型$\tilde f$；
	2. 从时间步$t=0$到任务总时长循环：
		1. 从CEM分布采样$N$次动作序列$a_{t:t+T}\sim CEM(\cdot)$；
		2. 对每个动作序列$a_{t:t+T}$循环：
			1. 从$p=0$到$P$，使用TS1和$\tilde f$传播状态粒子$s^p_\tau$；
			2. 评估动作序列（论文省略了折扣因子）
			$$
			\sum^{t+T}_{\tau=t}\frac1P\sum^P_{p=1}r(s^p_\tau,a_\tau)
			$$
			3. 更新CEM分布。
		3. 从最优动作序列$a_{t:t+T}^*$中选择第一个动作$a_t^*$执行；
		4. 将观测到的数据$\{s_t,a_t^*,s_{t+1}\}$放入$\mathbb D$。